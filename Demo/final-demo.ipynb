{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2eb9d0-3392-43ff-90d2-16646bc68122",
   "metadata": {},
   "source": [
    "### DocuTalk - AI : Advanced AI App for RAG over documents. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "987ea9da-3414-499e-9be0-e8539ad9fb7a",
   "metadata": {},
   "source": [
    "Aim: \n",
    "    DocuTalk - AI is a advanced AI for retrieving information from documents over vector db. \n",
    "It has following main components.\n",
    "1. Key value extractor - Extracting all the relevant information in form of key value phraser for quick document over\n",
    "2. Question Answering - Question answering with Vector DB with Large Language Models. \n",
    "\n",
    "Demo: Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb4efae-15be-411e-a2ca-ea75d3d62741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "\n",
    "# Huggingface \n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# langchain - RAG\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Gradio App\n",
    "import gradio as gr\n",
    "\n",
    "from docx import Document \n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f0c1b9-bcf2-4421-b134-de919d1d6165",
   "metadata": {},
   "source": [
    "#### Step by Step"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72599b33-b59f-4963-911a-87771b3c76a6",
   "metadata": {},
   "source": [
    "Step 1: Loading the sample text\n",
    "Step 2: Splitting the content\n",
    "Step 3: Making the vectors and storing in Vector db\n",
    "\n",
    "Step 4: NER model\n",
    "Step 5: Simple demo | Only NER\n",
    "\n",
    "Step 6: LLM model\n",
    "Step 7: Retrieving the answers from docs\n",
    "\n",
    "Step 8: Final Demo : Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bddded8-6b30-44e5-9592-26ddc2b0663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: \n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"sample.txt\")\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5599b9e-ef98-46d7-a0c2-dac5bfda9713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 675, which is longer than the specified 500\n",
      "Created a chunk of size 559, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "# Step 2:\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False)\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2dcd955-843a-4bd3-bc55-584610bb2f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eefc318-a8f1-49cc-999b-affe465f1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Making vectors and storing in vector db\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6219ae9c-6ae9-499b-b47e-fde6bf3b3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e528a3d-fadc-4eab-ae44-3a6d79745c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample : query \n",
    "query = \"Who is Yeshwanth Sai?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2fda4-c748-4b30-846b-670a1018fc0d",
   "metadata": {},
   "source": [
    "### Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21bec2fd-f24d-4624-b79a-afd1d4ab9896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# NER Model\n",
    "get_completion = pipeline(\"ner\", model=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fd7d97-05d6-47d6-9fdf-22b64907bd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def merge_tokens(tokens):\n",
    "    merged_tokens = []\n",
    "    for token in tokens:\n",
    "        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):\n",
    "            # If current token continues the entity of the last one, merge them\n",
    "            last_token = merged_tokens[-1]\n",
    "            last_token['word'] += token['word'].replace('##', '')\n",
    "            last_token['end'] = token['end']\n",
    "            last_token['score'] = (last_token['score'] + token['score']) / 2\n",
    "        else:\n",
    "            # Otherwise, add the token to the list\n",
    "                \n",
    "            merged_tokens.append(token)\n",
    "    merged_tokens_updated = []\n",
    "    for token in merged_tokens:\n",
    "        temp = {}\n",
    "        for k,v in token.items():\n",
    "            temp[k]=str(v)\n",
    "        merged_tokens_updated.append(temp)\n",
    "            \n",
    "    return merged_tokens_updated\n",
    "\n",
    "def ner(input):\n",
    "    output = get_completion(input)\n",
    "    merged_tokens = merge_tokens(output)\n",
    "    return {\"text\": input, \"entities\": merged_tokens}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3958a39-3386-4284-bece-0b664773f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_doc(f):\n",
    "    # load the contents of the file\n",
    "    path = f.name\n",
    "    path = path.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    \n",
    "    f = open(path, 'rb')\n",
    "    document = Document(f)\n",
    "    f.close()\n",
    "\n",
    "    content = \"\"\n",
    "    for para in document.paragraphs:\n",
    "        content += para.text\n",
    "    \n",
    "    # get ner\n",
    "    output = get_completion(content)\n",
    "    merged_tokens = merge_tokens(output)\n",
    "    return merged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "232905a7-900c-47d9-83ab-2ef4322a08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_ner_(f):\n",
    "    global document\n",
    "    global embedding_function\n",
    "    \n",
    "    # load the contents of the file\n",
    "    path = f.name\n",
    "    path = path.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    \n",
    "    f = open(path, 'rb')\n",
    "    document = Document(f)\n",
    "    f.close()\n",
    "\n",
    "    content = \"\"\n",
    "    for para in document.paragraphs:\n",
    "        content += para.text\n",
    "    \n",
    "    # get ner\n",
    "    output = get_completion(content)\n",
    "    merged_tokens = merge_tokens(output)\n",
    "    return merged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d8e4c5-1193-4c28-844e-3ecafb5ef98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_ner(f=None):\n",
    "\n",
    "    path = f.name\n",
    "    path = path.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    \n",
    "    f = open(path, 'rb')\n",
    "    document = Document(f)\n",
    "    f.close()\n",
    "\n",
    "    content = \"\"\n",
    "    for para in document.paragraphs:\n",
    "        content += para.text\n",
    "\n",
    "    # initializing the chroma db:\n",
    "    loader = Docx2txtLoader(path)\n",
    "    data = loader.load()\n",
    "    \n",
    "    \n",
    "    # split it into chunks\n",
    "    text_splitter = CharacterTextSplitter(separator=\"\\n\",\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False)\n",
    "    \n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    db = Chroma.from_documents(docs, embedding_function)\n",
    "    \n",
    "    # get ner\n",
    "    output = get_completion(content)\n",
    "    merged_tokens = merge_tokens(output)\n",
    "    return merged_tokens\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d678102-413f-48fe-ac5e-ea1ff86cafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: LLM model\n",
    "model_name = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0286bc4f-c785-4905-969b-507aa3c28713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG function\n",
    "def final_rag(question):\n",
    "    global db\n",
    "    global model, tokenizer\n",
    "\n",
    "    docs = db.similarity_search(question)\n",
    "    # print results\n",
    "    context = docs[0].page_content\n",
    "\n",
    "    prompt = f\"\"\"Role: You are an expert in answering questions. You have given a context below. Carefully analyze the context.  \n",
    "        #### Context: {context}\n",
    "        \n",
    "        Now answer the following question based on the above context:\n",
    "        #### Question: {question}\n",
    "        #### Answer:\n",
    "        \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    output = tokenizer.decode(model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=50)[0], \n",
    "        skip_special_tokens=True)\n",
    "    # framing the response\n",
    "\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a8f06ab-3b0e-4ddb-9615-2aa3c4af738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c3ce763-44b7-4490-a755-3cc0de71b42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:3001\n",
      "Running on public URL: https://30f94a86ed1a265ee6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://30f94a86ed1a265ee6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 675, which is longer than the specified 500\n",
      "Created a chunk of size 559, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "demo_ner = gr.Interface(fn=final_ner,\n",
    "                    inputs=[gr.File(label=\"Upload file to find entities\")],\n",
    "                    outputs=[gr.JSON(label=\"Entities\")],\n",
    "                    title=\"DocuTalk - AI\",\n",
    "                    description=\"Find key phrase information entities using the `dslim/bert-base-NER` model under the hood!\",\n",
    "                    allow_flagging=\"never\")\n",
    "\n",
    "demo_rag = gr.Interface(fn=final_rag,\n",
    "                    inputs=[gr.Textbox(label=\"Ask question over uploaded document\", lines=6)],\n",
    "                    outputs=[ gr.Textbox(label=\"Result\", lines=3)],\n",
    "                    title=\"DocuTalk - AI\",\n",
    "                    description=\"QA over documents : \",\n",
    "                    allow_flagging=\"never\")\n",
    "\n",
    "app = gr.TabbedInterface(interface_list=[demo_ner, demo_rag], \n",
    "                         tab_names = [\"Key Phrase information\", \"RAG over Docs\"])\n",
    "\n",
    "app.launch(share=True, server_port=int(3001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6df426-3fe0-4415-8141-53dcdf53d20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b504233-9184-4234-8d34-f8b57c97e76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3248d573-30db-4a3c-8052-e28d7c8d35ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
